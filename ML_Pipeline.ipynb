{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation_VK\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vitaliy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vitaliy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Vitaliy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vitaliy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger', 'stopwords'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score,recall_score, classification_report,make_scorer, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from scipy.stats import hmean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///twitter_messages.db')\n",
    "df = pd.read_sql('SELECT * FROM message',engine)\n",
    "\n",
    "X = df['message']\n",
    "y = df.iloc[:,4:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_test, y_pred):\n",
    "    res_metrics = []\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        f1 = f1_score(y_test[:,i],y_pred[:,i])\n",
    "        precision = precision_score(y_test[:,i],y_pred[:,i])\n",
    "        recall = recall_score(y_test[:,i],y_pred[:,i])\n",
    "        res_metrics.append([f1,precision,recall])\n",
    "    return res_metrics\n",
    "\n",
    "def median_f1(y_test,y_pred):\n",
    "    f1 = []\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        f1.append(f1_score(np.array(y_test)[:,i],y_pred[:,i]))\n",
    "    return np.median(f1)\n",
    "\n",
    "def f1_micro_average(y_test,y_pred):\n",
    "    TN = []\n",
    "    FP = []\n",
    "    FN = []\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        TN.append(confusion_matrix(np.array(y_test)[:,i],y_pred[:,i])[1,1])\n",
    "        FP.append(confusion_matrix(np.array(y_test)[:,i],y_pred[:,i])[1,0])\n",
    "        FN.append(confusion_matrix(np.array(y_test)[:,i],y_pred[:,i])[0,1])\n",
    "    precision = np.sum(TN) / (np.sum(TN) + np.sum(FN))\n",
    "    recall = np.sum(TN) / (np.sum(TN) + np.sum(FP))\n",
    "    \n",
    "    return hmean([precision,recall])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 1, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_Median: 0.14492753623188404\n",
      "F1_micro_average: 0.6152200488997556\n",
      "Time required: 226.63196277618408\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print('F1_Median: {}'.format(median_f1(y_test,y_pred)))\n",
    "print('F1_micro_average: {}'.format(f1_micro_average(y_test,y_pred)))\n",
    "print('Time required: {}'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on Test Data using untuned MultiOutputClassifier with Random Forest\n",
      "                              f1  precision    recall\n",
      "related                 0.882367   0.844965  0.923234\n",
      "request                 0.552764   0.822650  0.416216\n",
      "offer                   0.000000   0.000000  0.000000\n",
      "aid_related             0.663012   0.761848  0.586876\n",
      "medical_help            0.155844   0.642857  0.088670\n",
      "medical_products        0.144928   0.645161  0.081633\n",
      "search_and_rescue       0.179775   0.695652  0.103226\n",
      "security                0.021739   0.250000  0.011364\n",
      "military                0.131313   0.500000  0.075581\n",
      "water                   0.563636   0.807292  0.432961\n",
      "food                    0.692607   0.822171  0.598319\n",
      "shelter                 0.477237   0.821622  0.336283\n",
      "clothing                0.209524   0.785714  0.120879\n",
      "money                   0.055556   0.750000  0.028846\n",
      "missing_people          0.000000   0.000000  0.000000\n",
      "refugees                0.093458   0.714286  0.050000\n",
      "death                   0.251908   0.785714  0.150000\n",
      "other_aid               0.085714   0.532258  0.046610\n",
      "infrastructure_related  0.000000   0.000000  0.000000\n",
      "transport               0.117187   0.681818  0.064103\n",
      "buildings               0.272727   0.750000  0.166667\n",
      "electricity             0.078431   0.500000  0.042553\n",
      "tools                   0.000000   0.000000  0.000000\n",
      "hospitals               0.000000   0.000000  0.000000\n",
      "shops                   0.000000   0.000000  0.000000\n",
      "aid_centers             0.000000   0.000000  0.000000\n",
      "other_infrastructure    0.000000   0.000000  0.000000\n",
      "weather_related         0.714928   0.836364  0.624286\n",
      "floods                  0.498305   0.901840  0.344262\n",
      "storm                   0.507553   0.753363  0.382688\n",
      "fire                    0.258065   0.800000  0.153846\n",
      "earthquake              0.796804   0.879093  0.728601\n",
      "cold                    0.095238   1.000000  0.050000\n",
      "other_weather           0.141414   0.477273  0.083004\n",
      "direct_report           0.443047   0.754762  0.313551\n"
     ]
    }
   ],
   "source": [
    "test_res = calculate_metrics(y_test.values,y_pred)\n",
    "print('Result on Test Data using untuned MultiOutputClassifier with Random Forest')\n",
    "print(pd.DataFrame(test_res, columns = ['f1','precision','recall'], index = df.iloc[:,4:40].columns ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=1, score=0.59943273751737, total= 1.5min\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=1, score=0.5898971288192845, total= 1.5min\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=5, score=0.6229568955931952, total= 1.1min\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  5.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=5, score=0.6167781857491659, total= 1.1min\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  6.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=1, score=0.6278083854701653, total= 5.6min\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 12.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=1, score=0.6325805257312106, total= 5.5min\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 19.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=5, score=0.6482635796972396, total= 3.4min\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 23.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=5, score=0.6487622014161792, total= 3.5min\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 27.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=1, score=0.6248326701263456, total=  57.2s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 28.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=1, score=0.6190028915041347, total=  57.3s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=5, score=0.6416186009738918, total=  52.2s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=5, score=0.639580791041562, total=  51.0s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=1, score=0.6357993995985045, total= 2.9min\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=1, score=0.636129601706447, total= 2.9min\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=5, score=0.6516194113196672, total= 2.4min\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=5, score=0.6507511019159845, total= 2.4min\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=1, score=0.5949861354762039, total=  52.4s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=1, score=0.5953385044054522, total=  52.6s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=5, score=0.6054091923084014, total=  49.2s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=5, score=0.6097731882721741, total=  49.6s\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=1, score=0.5959847036328871, total= 2.4min\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=1, score=0.6042503219940905, total= 2.5min\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=5, score=0.6132219694992356, total= 2.2min\n",
      "[CV] clf__estimator__max_depth=None, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=None, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=5, score=0.6136667657550535, total= 2.2min\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=1, score=0.3740788504053058, total=  31.9s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=1, score=0.3680614662648662, total=  32.5s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=5, score=0.3819051761635494, total=  31.8s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=5, score=0.37954939341421146, total=  31.5s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=1, score=0.3733634519638576, total=  45.2s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=1, score=0.3648312119535141, total=  45.0s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=5, score=0.3808715596330275, total=  44.5s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=5, score=0.3806743008348921, total=  44.6s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=1, score=0.37880490881981876, total=  31.6s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=1, score=0.3716265834404259, total=  31.7s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=5, score=0.40009952724558345, total=  32.3s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=5, score=0.38213049024968104, total=  31.3s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=1, score=0.3723387078172214, total=  43.9s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=1, score=0.3678615802310064, total=  43.7s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=5, score=0.3878832150112996, total=  43.8s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=5, score=0.38086766114684545, total=  43.5s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=1, score=0.3779538408542887, total=  31.5s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=1, score=0.3751344670527111, total=  31.6s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=5, score=0.3862355384159003, total=  31.2s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=5, score=0.3860573973505419, total=  31.6s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=1, score=0.36872283362619074, total=  43.6s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=1, score=0.36474738856734384, total=  43.9s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=5, score=0.37994262765347103, total=  43.3s\n",
      "[CV] clf__estimator__max_depth=5, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=5, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=5, score=0.37448173176039395, total=  43.8s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=1, score=0.3991588087646699, total=  33.0s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=1, score=0.38880960471174536, total=  33.3s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=5, score=0.42970787112741415, total=  32.5s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, vect__min_df=5, score=0.41583982597500607, total=  32.7s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=1, score=0.38765494099116576, total=  51.7s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=1, score=0.3798242610977976, total=  51.5s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=5, score=0.42419121734296833, total=  49.8s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__min_df=5, score=0.41100786275911366, total=  50.1s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=1, score=0.3984334321870826, total=  33.6s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=1, score=0.3945219078561436, total=  32.2s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=5, score=0.4257772652681066, total=  32.2s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=10, vect__min_df=5, score=0.4249663897031274, total=  32.3s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=1, score=0.3846136254488074, total=  47.4s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=1, score=0.3813171031935991, total=  47.1s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=5, score=0.4165623323197997, total=  48.9s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=50, clf__estimator__n_estimators=50, vect__min_df=5, score=0.41131048027138006, total=  52.7s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=1, score=0.4010998918139199, total=  36.5s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=1, score=0.39665848551034655, total=  36.2s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=5, score=0.4286757061647017, total=  37.1s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=10, vect__min_df=5, score=0.41729106628242074, total=  36.7s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=1, score=0.3833050963871972, total=  50.3s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=1 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=1, score=0.3758582806921176, total=  54.3s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=5, score=0.41170005159148515, total=  52.5s\n",
      "[CV] clf__estimator__max_depth=10, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=5 \n",
      "[CV]  clf__estimator__max_depth=10, clf__estimator__min_samples_split=500, clf__estimator__n_estimators=50, vect__min_df=5, score=0.403408071748879, total=  54.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed: 109.4min finished\n"
     ]
    }
   ],
   "source": [
    "# Create grid search object\n",
    "\n",
    "parameters = {'vect__min_df': [1, 5],\n",
    "              'clf__estimator__n_estimators': [10, 50],\n",
    "             'clf__estimator__min_samples_split':[2,50,500],\n",
    "             'clf__estimator__max_depth':[None,5,10]}\n",
    "\n",
    "scorer = make_scorer(f1_micro_average)\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid = parameters, scoring = scorer, verbose = 10, cv = 2)\n",
    "\n",
    "tuned_model_rf = cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__min_samples_split': 50,\n",
       " 'clf__estimator__n_estimators': 50,\n",
       " 'vect__min_df': 5}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_model_rf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_Median: 0.18309859154929575\n",
      "F1_micro_average: 0.6686651256862177\n"
     ]
    }
   ],
   "source": [
    "print('F1_Median: {}'.format(median_f1(y_test,y_pred)))\n",
    "print('F1_micro_average: {}'.format(f1_micro_average(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on Test Data using tuned MultiOutputClassifier with Random Forest\n",
      "                              f1  precision    recall\n",
      "related                 0.891412   0.844200  0.944217\n",
      "request                 0.623218   0.837591  0.496216\n",
      "offer                   0.000000   0.000000  0.000000\n",
      "aid_related             0.728508   0.738717  0.718577\n",
      "medical_help            0.200820   0.597561  0.120690\n",
      "medical_products        0.183099   0.666667  0.106122\n",
      "search_and_rescue       0.072289   0.545455  0.038710\n",
      "security                0.000000   0.000000  0.000000\n",
      "military                0.196078   0.625000  0.116279\n",
      "water                   0.642612   0.834821  0.522346\n",
      "food                    0.750903   0.810916  0.699160\n",
      "shelter                 0.593620   0.795539  0.473451\n",
      "clothing                0.178218   0.900000  0.098901\n",
      "money                   0.019048   1.000000  0.009615\n",
      "missing_people          0.000000   0.000000  0.000000\n",
      "refugees                0.143498   0.695652  0.080000\n",
      "death                   0.407018   0.892308  0.263636\n",
      "other_aid               0.074271   0.608696  0.039548\n",
      "infrastructure_related  0.006309   0.166667  0.003215\n",
      "transport               0.252632   0.705882  0.153846\n",
      "buildings               0.267974   0.759259  0.162698\n",
      "electricity             0.020202   0.200000  0.010638\n",
      "tools                   0.000000   0.000000  0.000000\n",
      "hospitals               0.000000   0.000000  0.000000\n",
      "shops                   0.000000   0.000000  0.000000\n",
      "aid_centers             0.000000   0.000000  0.000000\n",
      "other_infrastructure    0.000000   0.000000  0.000000\n",
      "weather_related         0.795843   0.828439  0.765714\n",
      "floods                  0.680352   0.909804  0.543326\n",
      "storm                   0.722488   0.760705  0.687927\n",
      "fire                    0.037037   0.500000  0.019231\n",
      "earthquake              0.868449   0.890351  0.847599\n",
      "cold                    0.256410   0.882353  0.150000\n",
      "other_weather           0.065934   0.450000  0.035573\n",
      "direct_report           0.534121   0.793372  0.402572\n"
     ]
    }
   ],
   "source": [
    "y_pred = tuned_model_rf.predict(X_test)\n",
    "test_res = calculate_metrics(y_test.values,y_pred)\n",
    "print('Result on Test Data using tuned MultiOutputClassifier with Random Forest')\n",
    "print(pd.DataFrame(test_res, columns = ['f1','precision','recall'], index = df.iloc[:,4:40].columns ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tuned_model_, open('try.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('try.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
